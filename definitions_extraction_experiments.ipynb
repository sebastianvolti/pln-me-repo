{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "definitions-extraction-experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RmM_TYHdCyYx",
        "5GkUIR1I9s0Y",
        "FgdpV4GF8wbY",
        "o3Ky2g5h83_3",
        "FGt7O9NW9WjA",
        "4ZO_o1wS90df",
        "5J7uLfszA9gd",
        "Leew1FBegIad",
        "FGfYjfFsBEeU",
        "2BOvxdNvpBaP",
        "xHNaC8Dwtnnx",
        "QpyPAyoT_P5A"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentación sobre extracción de definición y definendum"
      ],
      "metadata": {
        "id": "gPKpMwBA9UlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets disponibles para experimentación"
      ],
      "metadata": {
        "id": "RmM_TYHdCyYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el siguiente repositorio público de github se encuentran los dataset utilizados en esta experimentación:\n",
        "\n",
        "*   https://github.com/sebastianvolti/pln-me-datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "fLXIrl30C2Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análisis manual de definiciones y pistas para crucigramas"
      ],
      "metadata": {
        "id": "5GkUIR1I9s0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En una primer etapa, realizamos el análisis manual de posibles definiciones y pistas para armar un crucigrama.\n",
        "\n",
        "Para realizar esta tarea, nos basamos en textos simples, extraídos de dos fuentes distintas de datos.\n",
        "\n",
        "*   Textos extraídos de https://lingua.com/english/reading/.\n",
        "*   Textos extraídos de dataset **“readworks”**\n",
        "\n",
        "Se obtuvieron 20 ejemplos manuales para cada fuente de datos, detallados en el siguiente documento:\n",
        "\n",
        "*   https://drive.google.com/file/d/1jplE-uB8wkzbQ-uTHtRtZ2CaCBCBgZ0D/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "jeNSeaVM-HW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ademas de las 2 fuentes de textos mencionadas anteriormente, se logró recolectar otro dataset formado por 96 textos de nivel básico de inglés, extraídos desde el siguiente sitio web:\n",
        "\n",
        "*  https://www.eslfast.com/kidsenglish/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iQL6wdYH-vCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análisis manual de algunos textos"
      ],
      "metadata": {
        "id": "FgdpV4GF8wbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Posibles tags (Part of speech):"
      ],
      "metadata": {
        "id": "o3Ky2g5h83_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **CD** cardinal digit\n",
        "* **DT** determiner\n",
        "* **EX** existential there (like: “there is” … think of it like “there exists”)\n",
        "* **FW** foreign word\n",
        "* **IN** preposition/subordinating conjunction\n",
        "* **JJ** adjective ‘big’\n",
        "* **JJR** adjective, comparative ‘bigger’\n",
        "* **JJS** adjective, superlative ‘biggest’\n",
        "* **LS** list marker 1)\n",
        "* **MD** modal could, will\n",
        "* **NN** noun, singular ‘desk’\n",
        "* **NNS** noun plural ‘desks’\n",
        "* **NNP** proper noun, singular ‘Harrison’\n",
        "* **NNPS** proper noun, plural ‘Americans’\n",
        "* **PDT** predeterminer ‘all the kids’\n",
        "* **POS** possessive ending parent’s\n",
        "* **PRP** personal pronoun I, he, she\n",
        "* **PRP** (con signo pesos) possessive pronoun my, his, hers\n",
        "* **RB** adverb very, silently,\n",
        "* **RBR** adverb, comparative better\n",
        "* **RBS** adverb, superlative best\n",
        "* **RP** particle give up\n",
        "* **TO**, to go ‘to’ the store.\n",
        "* **UH** interjection, errrrrrrrm\n",
        "* **VB** verb, base form take\n",
        "* **VBD** verb, past tense took\n",
        "* **VBG** verb, gerund/present participle taking\n",
        "* **VBN** verb, past participle taken\n",
        "* **VBP** verb, sing. present, non-3d take\n",
        "* **VBZ** verb, 3rd person sing. present takes\n",
        "* **WDT** wh-determiner which\n",
        "* **WP** wh-pronoun who, what\n",
        "* **WP$** possessive wh-pronoun whose\n",
        "* **WRB** wh-abverb where, when"
      ],
      "metadata": {
        "id": "u1yw8puq85ep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Oraciones analizadas, **pos tagging**:"
      ],
      "metadata": {
        "id": "FGt7O9NW9WjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ('People', 'NNS'), ('use', 'VBP'), **`('money', 'NN')`**, ('to', 'TO'), ('buy', 'VB'), ('things', 'NNS'), ('.', '.').\n",
        "* ('A', 'DT'), **`('bank', 'NN')`**, ('is', 'VBZ'), ('a', 'DT'), ('place', 'NN'), ('that', 'WDT'), ('keeps', 'VBZ'), ('money', 'NN'), ('safe', 'JJ'), ('.', '.')\n",
        "* ('Everything', 'VBG'), ('about', 'IN'), ('an', 'DT'), **`('elephant', 'NN')`**, ('is', 'VBZ'), ('big', 'JJ'), ('.', '.'), ('It', 'PRP'), ('has', 'VBZ'), ('big', 'JJ'), ('ears', 'NNS'), ('.', '.')\n",
        "* ('An', 'DT'), ('elephant', 'NN'), ('also', 'RB'), ('has', 'VBZ'), ('a', 'DT'), ('long', 'JJ'), **`('trunk', 'NN')`**, ('.', '.'), ('It', 'PRP'), ('uses', 'VBZ'), ('its', 'PRP$'), ('trunk', 'NN'), ('to', 'TO'), ('breathe', 'VB'), ('and', 'CC'), ('to', 'TO'), ('smell', 'VB'), ('.', '.')\n",
        "*  **`('Lightning', 'VBG')`**, ('is', 'VBZ'), ('electricity', 'NN'), ('.', '.'), ('It', 'PRP'), ('forms', 'VBZ'), ('in', 'IN'), ('clouds', 'NN'), ('during', 'IN'), ('a', 'DT'), ('storm', 'NN'), ('.', '.')\n",
        "* ('A', 'DT'), **`('desert', 'NN')`**, ('is', 'VBZ'), ('a', 'DT'), ('dry', 'JJ'), ('place', 'NN'), ('.', '.'), ('Very', 'RB'), ('little', 'JJ'), ('rain', 'NN'), ('falls', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('desert', 'NN'), ('.', '.')\n",
        "* ('Deserts', 'NNS'), ('are', 'VBP'), ('dry', 'JJ'), (',', ','), ('but', 'CC'), ('plants', 'NNS'), ('and', 'CC'), **`('animals', 'NNS')`**, ('find', 'VBP'), ('ways', 'NNS'), ('to', 'TO'), ('live', 'VB'), ('there', 'RB'), ('.', '.')\n",
        "* ('Deserts', 'NNS'), ('are', 'VBP'), ('dry', 'JJ'), (',', ','), ('but', 'CC'), **`('plants', 'NNS')`**, ('and', 'CC'), ('animals', 'NNS'), ('find', 'VBP'), ('ways', 'NNS'), ('to', 'TO'), ('live', 'VB'), ('there', 'RB'), ('.', '.')\n",
        "* **`('Cactus', 'NN')`**, ('plants', 'NNS'), ('grow', 'VB'), ('in', 'IN'), ('deserts', 'NNS'), ('.', '.')\n",
        "* **`('Camels', 'NNP')`**, ('live', 'VBP'), ('in', 'IN'), ('deserts', 'NNS'), ('.', '.')"
      ],
      "metadata": {
        "id": "TJlrw5mU9AU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Oraciones analizadas, **spicy named entities**:"
      ],
      "metadata": {
        "id": "qvRkdaqAxJG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* People use money to buy things. \n",
        "  * **spicy**: Nothing.\n",
        "* A bank is a place that keeps money safe.\n",
        "  * **spicy**: Nothing.\n",
        "* Everything about an elephant is big. It has big ears.\n",
        "  * **spicy**: Nothing.\n",
        "* An elephant also has a long trunk. It uses its trunk to breathe and to smell.\n",
        "  * **spicy**: Nothing.\n",
        "* Lightning is electricity. It forms in clouds during a storm. \n",
        "  * **spicy**: ('Lightning', 'PERSON').\n",
        "* A desert is a dry place. Very little rain falls in the desert.\n",
        "  * **spicy**: Nothing.\n",
        "* Deserts are dry, but plants and animals find ways to live there. \n",
        "  * **spicy**: Nothing.\n",
        "* Cactus plants grow in deserts.\n",
        "  * **spicy**: Nothing.\n",
        "* Camels live in deserts.\n",
        "  * **spicy**: Nothing."
      ],
      "metadata": {
        "id": "GIGoleUaxQK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementación"
      ],
      "metadata": {
        "id": "4ZO_o1wS90df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizaremos a continuación algunos experimentos con los textos provistos por el dataset **“readworks”**, formado por 368 textos en inglés de nivel básico."
      ],
      "metadata": {
        "id": "18WbRKfZ_vib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "csrjUgiVAUKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmVr8q2c0WFJ",
        "outputId": "8f180723-ce42-4064-812d-764743765e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pySsDGhXAz60",
        "outputId": "d18cd235-98a0-4045-e695-dd757306f9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "5J7uLfszA9gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_dataset_files(dataset_path):\n",
        "    all_texts = []\n",
        "    filenames = []\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        filenames.append(filename)\n",
        "        file_path = dataset_path + filename\n",
        "        if os.path.isfile(file_path):\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "                all_texts.append((text))\n",
        "    return all_texts, filenames\n",
        "\n",
        "def extract(all_texts):\n",
        "    all_text = \"\".join([t for t in all_texts])\n",
        "    all_tokens = nltk.word_tokenize(all_text)\n",
        "    return all_texts, all_tokens"
      ],
      "metadata": {
        "id": "T5YAMpZTg7Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.data\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def read_dataset_files_sentences(dataset_path):\n",
        "  all_texts_sentences = []\n",
        "  filenames = []\n",
        "  for filename in os.listdir(dataset_path):\n",
        "      filenames.append(filename)\n",
        "      file_path = dataset_path + filename\n",
        "      if os.path.isfile(file_path):\n",
        "          with open(file_path, 'r', encoding='utf-8') as file:\n",
        "              text = file.read()\n",
        "              sentences = tokenizer.tokenize(text)\n",
        "              all_texts_sentences.append((sentences))\n",
        "  return all_texts_sentences, filenames"
      ],
      "metadata": {
        "id": "-oTbg0wY8Qzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_pos_features(texts):\n",
        "  texts_pos = [[p for p in nltk.pos_tag(nltk.word_tokenize(text))] for text in texts]\n",
        "  return texts_pos\n",
        "\n",
        "def get_pos_features_sentences(texts):\n",
        "  texts_pos = []\n",
        "  for text in texts:\n",
        "    texts_pos_sentence = [[p for p in nltk.pos_tag(nltk.word_tokenize(sentence))] for sentence in text]\n",
        "    texts_pos.append(texts_pos_sentence)\n",
        "  return texts_pos\n"
      ],
      "metadata": {
        "id": "R-etJ_kX64BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_tuples(sent):\n",
        "  lists = list(map(list, zip(*sent)))\n",
        "  tokens = lists[0]\n",
        "  pos = lists[1]\n",
        "  return tokens, pos"
      ],
      "metadata": {
        "id": "v5CNa53pD40P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_named_entities_nltk(texts_pos):\n",
        "  texts_entities = []\n",
        "  pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "  cp = nltk.RegexpParser(pattern)\n",
        "  for sentence in texts_pos:\n",
        "    cs = cp.parse(sentence)\n",
        "    texts_entities.append(cs)\n",
        "  return texts_entities"
      ],
      "metadata": {
        "id": "ysXl5YtT8q06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "def get_text_named_entities_spicy(all_texts):\n",
        "  texts_entities = []\n",
        "  for text in all_texts:\n",
        "    doc = nlp(text)\n",
        "    texts_entities.append([(X.text, X.label_) for X in doc.ents])\n",
        "  return texts_entities"
      ],
      "metadata": {
        "id": "O_CGNRHx-S_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parser_nltk(tags):\n",
        "    chunkPattern = '\\n'.join([\n",
        "      'NP: {<JJ>*<NN>}',\n",
        "      'NP: {<NNP>+}',\n",
        "    ]) \n",
        "    chunkParser = nltk.RegexpParser(chunkPattern)\n",
        "    chunkedData = chunkParser.parse(tags)"
      ],
      "metadata": {
        "id": "PSdT_B4OT_S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parser_spicy(tags):\n",
        "    chunkPattern = '\\n'.join([\n",
        "    'NP: {<MONEY>+}',\n",
        "    ]) \n",
        "    chunkParser = nltk.RegexpParser(chunkPattern)\n",
        "    chunkedData = chunkParser.parse(tags)\n",
        "    return chunkedData"
      ],
      "metadata": {
        "id": "3rNr_ysWUFet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Patterns Functions"
      ],
      "metadata": {
        "id": "k6PyNtPVGRv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Utils"
      ],
      "metadata": {
        "id": "Leew1FBegIad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_index_list(elem_list, elem_value):\n",
        "  index = 0\n",
        "  index_list = []\n",
        "  for elem in elem_list:\n",
        "    if (elem == elem_value):\n",
        "      index_list.append(index)\n",
        "    index+=1\n",
        "  return index_list"
      ],
      "metadata": {
        "id": "Zt911JfIgNoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_index_list_v2(elem_list, elem_value_list):\n",
        "  index = 0\n",
        "  index_list = []\n",
        "  for elem in elem_list:\n",
        "    if (elem in elem_value_list):\n",
        "      index_list.append(index)\n",
        "    index+=1\n",
        "  return index_list"
      ],
      "metadata": {
        "id": "iX0Mhk5Z2M44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_professions = ['accountant','actor','actress','air traffic controller','architect','artist','attorney','banker','bartender','barber','bookkeeper','builder','businessman','businesswoman','businessperson','butcher','carpenter','cashier','chef','coach','dental hygienist','dentist','designer','developer','dietician','doctor','economist','editor','electrician','engineer','farmer','filmmaker','fisherman','flight attendant','jeweler','judge','lawyer','mechanic','musician','nutritionist','nurse','notary','optician','painter','pharmacist','photographer','physician','pilot','plumber','police officer','politician','professor','programmer','psychologist','receptionist','salesman','salesperson','saleswoman','secretary','singer','surgeon','teacher','therapist','translator','translator','undertaker','veterinarian','videographer','waiter','waitress','writer']\n",
        "\n",
        "def professions(token_list):\n",
        "  profs = []\n",
        "  for prof in list_of_professions:\n",
        "    if prof in token_list:\n",
        "      profs.append(prof)\n",
        "  return profs"
      ],
      "metadata": {
        "id": "IcNgJbiqgUST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_np(pos_list, nn):\n",
        "  if (nn > 0 and pos_list[nn-1] in ['PRP', 'PRP$']):\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "metadata": {
        "id": "XrKQJN-77gLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_nn(pos_list):\n",
        "  for pos in pos_list:\n",
        "    if pos in ['NN','NNS','NNP','NNPS']:\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "Bg0dg8Z_1OFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_np(pos_list):\n",
        "  index = 0\n",
        "  for pos in pos_list:\n",
        "    if pos in ['NN','NNS','NNP','NNPS']:\n",
        "      if index > 0 and pos_list[index-1] in ['PRP', 'PRP$']:\n",
        "        return True\n",
        "    index+=1\n",
        "  return False"
      ],
      "metadata": {
        "id": "ODsvxMrXDMZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_verb(pos_list):\n",
        "   for pos in pos_list:\n",
        "    if pos in ['VB','VBD','VBG','VBN', 'VBP', 'VBZ']:\n",
        "      return True\n",
        "   return False"
      ],
      "metadata": {
        "id": "Fdum-rFo5n3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contains_nnp(pos_list):\n",
        "   for pos in pos_list:\n",
        "    if pos in ['NNP', 'NNPS']:\n",
        "      return True\n",
        "   return False"
      ],
      "metadata": {
        "id": "HblxHyOq7ed3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_np_index(pos_list, nn_list):\n",
        "  np_list = []\n",
        "  for nn in nn_list:\n",
        "    if is_np(pos_list, nn):\n",
        "      np_list.append(nn)\n",
        "  return np_list "
      ],
      "metadata": {
        "id": "vBlhc0lF2wfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Patterns"
      ],
      "metadata": {
        "id": "vE75fHumgKHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "NP is a [list of professions] → NP is a **XXX**."
      ],
      "metadata": {
        "id": "-icoFXQ1GZJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PATTERN: PRP NN -> IS/VBZ -> [list_of_professions]\n",
        "def possible_combinations_p1(token_list, pos_list, nn_index, vbz_index, prof_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for nn in nn_index:\n",
        "    for vbz in vbz_index:\n",
        "      for prof in prof_list:\n",
        "        if nn < vbz and vbz < token_list.index(prof):\n",
        "          control_nn = [pos_list[elem] for elem in range(nn+1,vbz)]\n",
        "          if (not contains_nn(control_nn)):\n",
        "            clue = token_list[nn-1] + ' ' +  token_list[nn] + ' ' + token_list[vbz] + ' a ...'\n",
        "            goal = prof \n",
        "            clue_list.append(clue)\n",
        "            goal_list.append(goal)\n",
        "    \n",
        "  return clue_list, goal_list\n",
        "\n",
        "\n",
        "def pattern_1(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  prof_list = professions(token_list)\n",
        "  if len(prof_list) > 0 and contains_nn(pos_list) and 'VBZ' in pos_list:\n",
        "    #nn_index = generate_index_list(pos_list, 'NN')\n",
        "    nn_index = generate_index_list_v2(pos_list, ['NN','NNS','NNP','NNPS'])\n",
        "    np_index = generate_np_index(pos_list, nn_index)\n",
        "    vbz_index = generate_index_list(pos_list, 'VBZ')\n",
        "    return possible_combinations_p1(token_list, pos_list, np_index, vbz_index, prof_list) \n",
        "  return clue_list, goal_list\n"
      ],
      "metadata": {
        "id": "Btd3xx0XGW5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NP1 is a [list of professions], NP1 works at NP2 → **XXX** works at NP2, NP1 works at **XXX**."
      ],
      "metadata": {
        "id": "L3IbilDWGcub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PATTERN: PRP NN1 -> IS/VBZ -> [list_of_professions], PRP NN1 -> IN -> NN2\n",
        "def possible_combinations_p2(token_list, pos_list, nn_index, vbz_index, in_index, prof_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for nn1 in nn_index:\n",
        "    for nn2 in nn_index:\n",
        "      if nn1 != nn2 and nn1 < nn2:\n",
        "        for vbz in vbz_index:\n",
        "          for in_i in in_index:\n",
        "            for prof in prof_list:       \n",
        "              if nn1 < vbz and is_np(pos_list, nn1) and vbz < token_list.index(prof) and vbz < in_i and token_list.index(prof) < nn2:\n",
        "                  control_nn = [pos_list[elem] for elem in range(nn1+1,in_i)]\n",
        "                  if (not contains_np(control_nn)):\n",
        "                    clue = token_list[nn1-1] + ' ' + token_list[nn1] + ' works ' + token_list[in_i] + ' ...'\n",
        "                    goal = token_list[nn2] \n",
        "                    clue_list.append(clue)\n",
        "                    goal_list.append(goal)\n",
        "                    clue = '... works ' + token_list[in_i] + ' ' + token_list[nn2]\n",
        "                    goal = token_list[nn1-1] + ' ' + token_list[nn1]  \n",
        "                    clue_list.append(clue)\n",
        "                    goal_list.append(goal)\n",
        "  return clue_list, goal_list\n",
        "\n",
        "def pattern_2(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  prof_list = professions(token_list)\n",
        "  if len(prof_list) > 0 and contains_nn(pos_list) and 'IN' in pos_list and 'VBZ' in pos_list:\n",
        "    #nn_index = generate_index_list(pos_list, 'NN')\n",
        "    nn_index = generate_index_list_v2(pos_list, ['NN','NNS','NNP','NNPS'])\n",
        "    vbz_index = generate_index_list(pos_list, 'VBZ')\n",
        "    in_index = generate_index_list(pos_list, 'IN')\n",
        "    if len(nn_index) > 1:\n",
        "      return possible_combinations_p2(token_list, pos_list, nn_index, vbz_index, in_index, prof_list) \n",
        "  return clue_list, goal_list"
      ],
      "metadata": {
        "id": "m6vPSMT6GePK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NP is XXX, PRONOUN is YYY → NP is **…YYY…**"
      ],
      "metadata": {
        "id": "60Yk-Bw8Gely"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PATTERN: PRP NN1 -> IS/VBZ1 -> NN2, PRP -> IS/VBZ2 -> NN3\n",
        "def possible_combinations_p3(token_list, pos_list, nn_index, vbz_index, prp_index):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for nn1 in nn_index:\n",
        "    for nn2 in nn_index:\n",
        "      for nn3 in nn_index:\n",
        "        if nn1 != nn2 != nn3 and nn1 < nn2 < nn3:\n",
        "          for vbz1 in vbz_index:\n",
        "            for vbz2 in vbz_index:\n",
        "              if vbz1 != vbz2 and vbz1 < vbz2:\n",
        "                for prp in prp_index:\n",
        "                  if nn1 < vbz1 and is_np(pos_list, nn1) and vbz1 < nn2 and nn2 < prp and prp < vbz2 and vbz2 < nn3:\n",
        "                    clue = token_list[nn1-1] + ' ' + token_list[nn1] + ' ' + token_list[vbz1] + ' ...'\n",
        "                    goal = token_list[nn3] \n",
        "                    clue_list.append(clue)\n",
        "                    goal_list.append(goal)\n",
        "  return clue_list, goal_list\n",
        "\n",
        "def pattern_3(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  if contains_nn(pos_list) and 'VBZ' in pos_list and 'PRP' in pos_list:\n",
        "    #nn_index = generate_index_list(pos_list, 'NN')\n",
        "    nn_index = generate_index_list_v2(pos_list, ['NN','NNS','NNP','NNPS'])\n",
        "    vbz_index = generate_index_list(pos_list, 'VBZ')\n",
        "    prp_index = generate_index_list(pos_list, 'PRP')\n",
        "    if len(nn_index) > 2 and len(vbz_index) > 1:\n",
        "      return possible_combinations_p3(token_list, pos_list, nn_index, vbz_index, prp_index) \n",
        "  return clue_list, goal_list"
      ],
      "metadata": {
        "id": "_pHiDr5bGfeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NP VERB XXX and YYY → NP VERB …XXX…, NP VERB …YYY…, not VERB in …XXX…, …YYY…"
      ],
      "metadata": {
        "id": "tFRYtJVMzp0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PATTERN: PRP NN1/NNS1 VERB XXX CC YYY -> PRP NN1/NNS1 VERB ...YYY..., not VERB in …XXX…, …YYY…\n",
        "def possible_combinations_p4(token_list, pos_list, np_index, verb_index, cc_index):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for np in np_index:\n",
        "    for verb in verb_index:\n",
        "      for cc in cc_index:\n",
        "        if ((np < verb < cc) and (cc - verb > 0)):\n",
        "          xxx_pos = [pos_list[elem] for elem in range(verb+1,cc)]\n",
        "          yyy_pos = [pos_list[elem] for elem in range(cc+1,len(token_list))]\n",
        "\n",
        "          xxx = [token_list[elem] for elem in range(verb+1,cc)]\n",
        "          yyy = [token_list[elem] for elem in range(cc+1,len(token_list)-1)]\n",
        "       \n",
        "          if (not contains_verb(xxx_pos) and not contains_verb(yyy_pos) and not contains_nnp(xxx_pos) and not contains_nnp(yyy_pos)):\n",
        "            clue = token_list[np-1] + ' ' + token_list[np] + ' ' + token_list[verb] + ' ...'\n",
        "            goal = \"\"\n",
        "            for elem in yyy:\n",
        "              goal = goal + elem + \" \"\n",
        "            clue_list.append(clue)\n",
        "            goal_list.append(goal)\n",
        "\n",
        "           \n",
        "            clue_inverse = \"\"\n",
        "            for elem in yyy:\n",
        "              clue_inverse = clue_inverse + elem + \" \"\n",
        "\n",
        "            clue = token_list[np-1] + ' ' + token_list[np] + ' ' + token_list[verb] + ' ' + clue_inverse + token_list[cc] + ' ...'\n",
        "            goal = \"\"\n",
        "            for elem in xxx:\n",
        "              goal = goal + elem + \" \"\n",
        "\n",
        "            clue_list.append(clue)\n",
        "            goal_list.append(goal)\n",
        "\n",
        "      \n",
        "\n",
        "  return clue_list, goal_list\n",
        "\n",
        "def pattern_4(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  if contains_nn(pos_list) and contains_verb(pos_list) and 'CC' in pos_list:\n",
        "    nn_index = generate_index_list_v2(pos_list, ['NN','NNS','NNP','NNPS'])\n",
        "    np_index = generate_np_index(pos_list, nn_index)\n",
        "    verb_index = generate_index_list_v2(pos_list, ['VB','VBD','VBG','VBN', 'VBP', 'VBZ'])\n",
        "    cc_index = generate_index_list(pos_list, 'CC')\n",
        "    return possible_combinations_p4(token_list, pos_list, np_index, verb_index, cc_index) \n",
        "  return clue_list, goal_list"
      ],
      "metadata": {
        "id": "8-4t18Bszqh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " NP1 is called NP2 →  NP2 is **…XXX…**, NP1 is **…XXX…**"
      ],
      "metadata": {
        "id": "nhFCvHPFG_OK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PATTERN: NN VBZ/VBP called NNP/NNPS\n",
        "def possible_combinations_p5(token_list, pos_list, nn_index, np_index, v_index, called_index):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for nn in nn_index:\n",
        "    for np in np_index:\n",
        "      for v in v_index:\n",
        "        for c in called_index:\n",
        "          if (nn < v < c < np):\n",
        "              clue = token_list[nn-1] + ' ' + token_list[nn] + ' ' + token_list[v] + ' ...'\n",
        "              goal = token_list[np-1] + ' ' + token_list[np]\n",
        "              clue_list.append(clue)\n",
        "              goal_list.append(goal)\n",
        "\n",
        "              clue = token_list[np-1]  + ' ' + token_list[np] + ' ' + token_list[v] + ' ...'\n",
        "              goal = token_list[nn-1] + ' ' + token_list[nn]\n",
        "              clue_list.append(clue)\n",
        "              goal_list.append(goal)\n",
        "  return clue_list, goal_list\n",
        "\n",
        "def pattern_5(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  if contains_nn(pos_list) and contains_nnp(pos_list) and ('VBZ' in pos_list or 'VBP' in pos_list) and contains_verb(pos_list) and 'called' in token_list:\n",
        "    nn_index = generate_index_list_v2(pos_list, ['NN','NNS','NNP','NNPS'])\n",
        "    np_index = generate_index_list_v2(pos_list, ['NNP','NNPS'])\n",
        "    v_index = generate_index_list_v2(pos_list, ['VBP','VBZ'])\n",
        "    called_index = generate_index_list(token_list, 'called')\n",
        "    return possible_combinations_p5(token_list, pos_list, nn_index, np_index, v_index, called_index) \n",
        "  return clue_list, goal_list"
      ],
      "metadata": {
        "id": "kT9AhhryG-ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NP1 like/likes NP2\n"
      ],
      "metadata": {
        "id": "kQEdfgiM9OR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#NP1 like/likes NP2 -> What NP1 likes.\n",
        "def possible_combinations_p6(token_list, pos_list, nn_index, like_index):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for nn1 in nn_index:\n",
        "    for nn2 in nn_index:\n",
        "      for like in like_index:\n",
        "        if (nn1 < like < nn2):\n",
        "            if (is_np(pos_list, nn1)):\n",
        "              nn1_token = token_list[nn1-1] + ' ' + token_list[nn1] + ' '\n",
        "            else:\n",
        "              nn1_token = token_list[nn1] + ' '\n",
        "            clue = 'What ' + nn1_token +  'likes'\n",
        "            goal = token_list[nn2]\n",
        "            clue_list.append(clue)\n",
        "            goal_list.append(goal)\n",
        "  return clue_list, goal_list\n",
        "\n",
        "def pattern_6(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  if contains_nn(pos_list) and ('like' in token_list or 'likes' in token_list):\n",
        "    nn_index = generate_index_list_v2(pos_list, ['NN','NNS','NNP','NNPS'])\n",
        "    like_index = generate_index_list_v2(token_list, ['like','likes'])\n",
        "    if (len(nn_index) > 1):\n",
        "      return possible_combinations_p6(token_list, pos_list, nn_index, like_index) \n",
        "  return clue_list, goal_list"
      ],
      "metadata": {
        "id": "6ZqtMSDX9OrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NP1 VERB NP2 xxx"
      ],
      "metadata": {
        "id": "2j80j9gjBQYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PRP NN1/NNS1 VERB PRP NN2/NNS2 xxx -> What PRP NN1/NNS1 VERB xxx.\n",
        "def possible_combinations_p7(token_list, pos_list, np_index, verb_index):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for np1 in np_index:\n",
        "    for np2 in np_index:\n",
        "      for verb in verb_index:\n",
        "          if (np1 < verb < np2):\n",
        "            xxx = [token_list[elem] for elem in range(np2+1,len(token_list)-1)]  \n",
        "            final = \" \"   \n",
        "            for elem in xxx:\n",
        "              final = final + elem + \" \"     \n",
        "            clue = 'What ' + token_list[np1-1] + ' ' + token_list[np1] + ' ' + token_list[verb] + final\n",
        "            goal = token_list[np2-1] + ' ' + token_list[np2]\n",
        "            clue_list.append(clue)\n",
        "            goal_list.append(goal)\n",
        "  return clue_list, goal_list\n",
        "\n",
        "def pattern_7(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  if contains_nn(pos_list) and contains_verb(pos_list) and 'CC' in pos_list:\n",
        "    nn_index = generate_index_list_v2(pos_list, ['NN','NNS','NNP','NNPS'])\n",
        "    np_index = generate_np_index(pos_list, nn_index)\n",
        "    verb_index = generate_index_list_v2(pos_list, ['VB','VBD','VBG','VBN', 'VBP', 'VBZ'])\n",
        "    if (len(np_index) > 1):\n",
        "      return possible_combinations_p7(token_list, pos_list, np_index, verb_index) \n",
        "  return clue_list, goal_list"
      ],
      "metadata": {
        "id": "JfMV9cLEBQtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " I live/lives in a house XXX -> My house is XXX"
      ],
      "metadata": {
        "id": "B6nhU_o3Jbcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PATTERN: PRP live -> IN -> house -> XXX \n",
        "def possible_combinations_p8(token_list, pos_list, prp_index, in_index, liv_index, hs_index):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for prp in prp_index:\n",
        "    for liv in liv_index:\n",
        "      for inn in in_index:\n",
        "        for hs in hs_index:\n",
        "          if (prp < liv < inn < hs):\n",
        "            xxx = [token_list[elem] for elem in range(hs+1,len(token_list)-1)]  \n",
        "            goal = ''\n",
        "            clue = 'My house is ...'\n",
        "            for elem in xxx:\n",
        "              goal = goal + elem + \" \" \n",
        "            clue_list.append(clue)\n",
        "            goal_list.append(goal)\n",
        "  return clue_list, goal_list\n",
        "\n",
        "def pattern_8(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  if 'PRP' in pos_list and 'IN' in pos_list  and 'live' in token_list and 'house' in token_list:\n",
        "    prp_index = generate_index_list(pos_list, 'PRP')\n",
        "    in_index = generate_index_list(pos_list, 'IN')\n",
        "    liv_index = generate_index_list(token_list, 'live')   \n",
        "    hs_index = generate_index_list(token_list, 'house')   \n",
        "    return possible_combinations_p8(token_list, pos_list, prp_index, in_index, liv_index, hs_index) \n",
        "  return clue_list, goal_list"
      ],
      "metadata": {
        "id": "HOzeXsG6Jb0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " NP live/lives in a house XXX -> NP’s house is XXX"
      ],
      "metadata": {
        "id": "Gs4rB05bH7Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PATTERN: PRP NN -> live/lives in -> house -> XXX \n",
        "def possible_combinations_p9(token_list, pos_list, np_index, in_index, liv_index, hs_index):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for np in np_index:\n",
        "    for liv in liv_index:\n",
        "      for inn in in_index:\n",
        "        for hs in hs_index:\n",
        "          if (np < liv < inn < hs):\n",
        "            xxx = [token_list[elem] for elem in range(hs+1,len(token_list)-1)]  \n",
        "            goal = ''\n",
        "            clue = token_list[np-1] + ' ' + token_list[np]  + ' house is ' \n",
        "            for elem in xxx:\n",
        "              goal = goal + elem + \" \" \n",
        "            clue_list.append(clue)\n",
        "            goal_list.append(goal)\n",
        "  return clue_list, goal_list\n",
        "\n",
        "def pattern_9(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  if contains_nn(pos_list) and contains_nnp(pos_list) and 'IN' in pos_list and ('live' in token_list or 'lives' in token_list) and 'house' in token_list:\n",
        "    nn_index = generate_index_list_v2(pos_list, ['NN','NNS','NNP','NNPS'])\n",
        "    np_index = generate_index_list_v2(pos_list, ['NNP','NNPS'])\n",
        "    in_index = generate_index_list(pos_list, 'IN')\n",
        "    liv_index = generate_index_list_v2(token_list, ['live','lives'])\n",
        "    hs_index = generate_index_list(token_list, 'house')   \n",
        "    return possible_combinations_p9(token_list, pos_list, np_index, in_index, liv_index, hs_index) \n",
        "  return clue_list, goal_list"
      ],
      "metadata": {
        "id": "LgbeOoWaHjT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NP live/lives in XXX -> NP’s house is in XXX "
      ],
      "metadata": {
        "id": "zKpxI5BKN2Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PATTERN: PRP NN -> live/lives in XXX, house NOT in XXX -> PRP NN house is in...\n",
        "def possible_combinations_p10(token_list, pos_list, np_index, in_index, liv_index):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  for np in np_index:\n",
        "    for liv in liv_index:\n",
        "      for inn in in_index:\n",
        "        if (np < liv < inn):\n",
        "          xxx = [token_list[elem] for elem in range(inn+1,len(token_list)-1)]  \n",
        "          if 'house' not in xxx:\n",
        "            goal = ''\n",
        "            clue = token_list[np-1] + ' ' + token_list[np] + ' house is ...' \n",
        "            for elem in xxx:\n",
        "              goal = goal + elem + \" \" \n",
        "            clue_list.append(clue)\n",
        "            goal_list.append(goal)\n",
        "  return clue_list, goal_list\n",
        "\n",
        "def pattern_10(token_list, pos_list, tuple_list):\n",
        "  clue_list = []\n",
        "  goal_list = []\n",
        "  if contains_nn(pos_list) and contains_nnp(pos_list) and 'IN' in pos_list and ('live' in token_list or 'lives' in token_list):\n",
        "    nn_index = generate_index_list_v2(pos_list, ['NN','NNS','NNP','NNPS'])\n",
        "    np_index = generate_index_list_v2(pos_list, ['NNP','NNPS'])\n",
        "    in_index = generate_index_list(pos_list, 'IN')\n",
        "    liv_index = generate_index_list_v2(token_list, ['live','lives'])\n",
        "    return possible_combinations_p10(token_list, pos_list, np_index, in_index, liv_index) \n",
        "  return clue_list, goal_list"
      ],
      "metadata": {
        "id": "aqv2qiMkOFST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Execution"
      ],
      "metadata": {
        "id": "FGfYjfFsBEeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar, procesamos cada uno de los textos de la siguiente manera:  \n",
        "* Obtenemos todos los **tokens** o palabras que los textos contienen.\n",
        "* Aplicamos **POS tagging** para cada texto, obteniendo el **tag** adecuado para cada **token**.\n",
        "* Obtenemos las **entidades con nombre** para cada texto, utilizando **nltk**.\n",
        "* Obtenemos las **entidades con nombre** para cada texto, utilizando **spicy**."
      ],
      "metadata": {
        "id": "6ErejUTRArL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/Semestre Impar 2022/Modulos PLN/ME/dataset-lingua/\"\n",
        "all_texts, filenames = read_dataset_files(DATASET_PATH)\n",
        "all_texts, all_tokens = extract(all_texts)\n",
        "texts_pos = get_pos_features(all_texts)\n",
        "texts_named_entities_nltk = get_text_named_entities_nltk(texts_pos)\n",
        "texts_named_entities_spicy = get_text_named_entities_spicy(all_texts)"
      ],
      "metadata": {
        "id": "iBvNbJRGj9uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracción de patrones"
      ],
      "metadata": {
        "id": "Qsc7L0hmop8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En base al análisis manual de definiciones y pistas para crucigramas, la idea es construir patrones adecuados para lograr realizar la extracción automática de definiciones a partir de textos.\n",
        "\n",
        "También nos basaremos en el proyecto de grado realizado por Esteche y Romero en 2015."
      ],
      "metadata": {
        "id": "A2IpI7tfoqwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Posibles patrones extraídos"
      ],
      "metadata": {
        "id": "2BOvxdNvpBaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Presentamos a continuación una lista con posibles patrones a utilizar para intentar extraer de cierto corpus, oraciones que contengan un par definición/definendum, con los cuales intentar armar pistas para crucigramas.  \n",
        "Los patrones fueron extraídos de los ejemplos analizados previamente de https://lingua.com/english/reading/\n"
      ],
      "metadata": {
        "id": "-M4LFaXrpFh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* NP VERB **…XXX…**\n",
        "* NP is a [list of professions] → NP is a **XXX**. \n",
        "* NP1 is a [list of professions], NP1 works at NP2 → **XXX** works at NP2, NP1 works at **XXX**.\n",
        "* NP1 VERB NP2 →  NP1 **…XXX…** NP2, **…XXX…** VERB NP2, NP1 VERB **…XXX…**\n",
        "* NP is XXX, PRONOUN is YYY → NP is **…YYY…**\n",
        "* NP VERB XXX and YYY → NP VERB **…XXX…**, NP VERB **…YYY…**, not VERB in …XXX…, …YYY… \n",
        "* NP1 is called NP2 →  NP2 is **…XXX…**, NP1 is **…XXX…**\n",
        "* **XXX** there [are, is] YYY in NP → Place where there [are, is] NP.\n",
        "* **XXX** NP1 VERB in NP2 →  Place where NP1 VERB.\n",
        "* [NP,he,she] is NUMBER → [NP,he,she] is **XXX** years old.\n",
        "* NP … . [he, she] is NUMBER → NP is **XXX** years old.\n",
        "* NP … . [he, she] is …XXX… → NP is **XXX**.\n",
        "* NP1 VERB1 …then… NP2 VERB2 → After NP1 VERB1 NP2 **XXX**, Before NP2 VERB2, NP1 **XXX**.\n",
        "* NP1 [like, likes] NP2 → What NP1 likes. \n",
        "* NP1 VERB NP2 … → What NP1 VERB **XXX**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gqDGIuDEpeR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicación de patrones"
      ],
      "metadata": {
        "id": "NuegenPqtfiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ejemplo de aplicación"
      ],
      "metadata": {
        "id": "xHNaC8Dwtnnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostraremos a continuación algunos ejemplos para ilustrar lo que queremos lograr, trabajando por ejemplo con el siguiente texto:"
      ],
      "metadata": {
        "id": "FkaR1p8rtvZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Texto 1: My Wonderful Family**\n",
        "\n",
        "I live in a house near the mountains. I have two brothers and one sister, and I was born last. My father teaches mathematics, and my mother is a nurse at a big hospital. My brothers are very smart and work hard in school. My sister is a nervous girl, but she is very kind. My grandmother also lives with us. She came from Italy when I was two years old. She has grown old, but she is still very strong. She cooks the best food!\n",
        "\n",
        "My family is very important to me. We do lots of things together. My brothers and I like to go on long walks in the mountains. My sister likes to cook with my grandmother. On the weekends we all play board games together. We laugh and always have a good time. I love my family very much."
      ],
      "metadata": {
        "id": "9vP1Af9g2aLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos los siguientes patrones:\n",
        "* NP is a [list of professions] → NP is a **XXX**.\n",
        "* NP1 is a [list of professions], NP1 works at NP2 → **XXX** works at NP2, NP1 works at **XXX**.\n",
        "* NP is XXX, PRONOUN is YYY → NP is **…YYY…**\n",
        "* NP VERB XXX and YYY → NP VERB **…XXX…**, NP VERB **…YYY…**, not VERB in …XXX…, …YYY… \n",
        "* NP1 is called NP2 →  NP2 is **…XXX…**, NP1 is **…XXX…**\n",
        "* NP1 like/likes NP2\n",
        "* NP1 VERB NP2 xxx\n",
        "* I live/lives in a house XXX -> My house is XXX\n",
        "* NP live/lives in a house XXX -> NP’s house is XXX\n",
        "* NP live/lives in XXX -> NP’s house is in XXX "
      ],
      "metadata": {
        "id": "IN7foWs-2X5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/Semestre Impar 2022/Modulos PLN/ME/dataset-lingua/\"\n",
        "all_texts_sentences, filenames = read_dataset_files_sentences(DATASET_PATH)\n",
        "texts_pos_sentences = get_pos_features_sentences(all_texts_sentences)"
      ],
      "metadata": {
        "id": "l4QXdZZG6xSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text_sentences in texts_pos_sentences:\n",
        "  print(\"Text \" + str(texts_pos_sentences.index(text_sentences) + 1) + \":\")\n",
        "  for pos_sent in text_sentences:\n",
        "    tokens, pos = separate_tuples(pos_sent)\n",
        "    clue_list_p1 = []\n",
        "    goal_list_p1 = []\n",
        "    clue_list_p1, goal_list_p1 = pattern_1(tokens, pos, pos_sent)\n",
        "    clue_list_p2, goal_list_p2 = pattern_2(tokens, pos, pos_sent)\n",
        "    clue_list_p3, goal_list_p3 = pattern_3(tokens, pos, pos_sent)\n",
        "    clue_list_p4, goal_list_p4 = pattern_4(tokens, pos, pos_sent)\n",
        "    clue_list_p5, goal_list_p5 = pattern_5(tokens, pos, pos_sent)\n",
        "    clue_list_p6, goal_list_p6 = pattern_6(tokens, pos, pos_sent)\n",
        "    clue_list_p7, goal_list_p7 = pattern_7(tokens, pos, pos_sent)\n",
        "    clue_list_p8, goal_list_p8 = pattern_8(tokens, pos, pos_sent)\n",
        "    clue_list_p9, goal_list_p9 = pattern_9(tokens, pos, pos_sent)\n",
        "    clue_list_p10, goal_list_p10 = pattern_10(tokens, pos, pos_sent)\n",
        "    clue_list = [*clue_list_p1, *clue_list_p2, *clue_list_p3, *clue_list_p4, *clue_list_p5, *clue_list_p6, *clue_list_p7, *clue_list_p8, *clue_list_p9, *clue_list_p10] \n",
        "    goal_list = [*goal_list_p1, *goal_list_p2, *goal_list_p3, *goal_list_p4, *goal_list_p5, *goal_list_p6, *goal_list_p7, *goal_list_p8, *goal_list_p9, *goal_list_p10]  \n",
        "    if len(clue_list) > 0 and len(goal_list) > 0:\n",
        "      print(\" Clues sentence \" + str(text_sentences.index(pos_sent)) + \": \" + str(clue_list))\n",
        "      print(\" Goals sentence \" + str(text_sentences.index(pos_sent)) + \": \" + str(goal_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxCeho2Q4Nl8",
        "outputId": "9d879277-d420-4cca-8f8e-5fe6ba0b1ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1:\n",
            " Clues sentence 0: ['My house is ...']\n",
            " Goals sentence 0: ['near the mountains ']\n",
            " Clues sentence 2: ['my mother is a ...', 'my mother works at ...', '... works at hospital']\n",
            " Goals sentence 2: ['nurse', 'hospital', 'my mother']\n",
            " Clues sentence 4: ['My sister is ...']\n",
            " Goals sentence 4: ['kind']\n",
            " Clues sentence 11: ['What My brothers likes', 'What My brothers likes']\n",
            " Goals sentence 11: ['walks', 'mountains']\n",
            " Clues sentence 12: ['What My sister likes']\n",
            " Goals sentence 12: ['grandmother']\n",
            "Text 2:\n",
            " Clues sentence 7: ['My professors are ...', 'My professors are smart and ...']\n",
            " Goals sentence 7: ['smart ', 'very friendly ']\n",
            " Clues sentence 10: ['My house is ...']\n",
            " Goals sentence 10: ['on Ivy Street ']\n",
            " Clues sentence 22: ['What My Mom brings and candy when they come ']\n",
            " Goals sentence 22: ['me sweets']\n",
            "Text 3:\n",
            " Clues sentence 2: ['favorite beach is ...', 'called Emerson is ...', 'favorite beach is ...', 'Emerson Beach is ...']\n",
            " Goals sentence 2: ['called Emerson', 'favorite beach', 'Emerson Beach', 'favorite beach']\n",
            "Text 4:\n",
            "Text 5:\n",
            " Clues sentence 3: ['The Smiths house is ']\n",
            " Goals sentence 3: ['']\n",
            " Clues sentence 16: ['What John likes', 'What Sarah likes']\n",
            " Goals sentence 16: ['dog', 'dog']\n",
            "Text 6:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ejecutamos patrones sobre dataset \"eslfast\""
      ],
      "metadata": {
        "id": "QpyPAyoT_P5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este dataset está formado por textos similares a los utilizados en el ejemplo de aplicación anterior."
      ],
      "metadata": {
        "id": "FfmVgSbF_ysV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/Semestre Impar 2022/Modulos PLN/ME/dataset-eslfast/\"\n",
        "all_texts_sentences, filenames = read_dataset_files_sentences(DATASET_PATH)\n",
        "texts_pos_sentences = get_pos_features_sentences(all_texts_sentences)"
      ],
      "metadata": {
        "id": "qrUziW3h_PfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text_sentences in texts_pos_sentences:\n",
        "  print(\"Text \" + str(texts_pos_sentences.index(text_sentences) + 1) + \":\")\n",
        "  for pos_sent in text_sentences:\n",
        "    tokens, pos = separate_tuples(pos_sent)\n",
        "    clue_list_p1, goal_list_p1 = pattern_1(tokens, pos, pos_sent)\n",
        "    clue_list_p2, goal_list_p2 = pattern_2(tokens, pos, pos_sent)\n",
        "    clue_list_p3, goal_list_p3 = pattern_3(tokens, pos, pos_sent)\n",
        "    clue_list_p4, goal_list_p4 = pattern_4(tokens, pos, pos_sent)\n",
        "    clue_list_p5, goal_list_p5 = pattern_5(tokens, pos, pos_sent)\n",
        "    clue_list_p6, goal_list_p6 = pattern_6(tokens, pos, pos_sent)\n",
        "    clue_list_p7, goal_list_p7 = pattern_7(tokens, pos, pos_sent)\n",
        "    clue_list_p8, goal_list_p8 = pattern_8(tokens, pos, pos_sent)\n",
        "    clue_list_p9, goal_list_p9 = pattern_9(tokens, pos, pos_sent)\n",
        "    clue_list_p10, goal_list_p10 = pattern_10(tokens, pos, pos_sent)\n",
        "    clue_list = [*clue_list_p1, *clue_list_p2, *clue_list_p3, *clue_list_p4, *clue_list_p5, *clue_list_p6, *clue_list_p7, *clue_list_p8, *clue_list_p9, *clue_list_p10] \n",
        "    goal_list = [*goal_list_p1, *goal_list_p2, *goal_list_p3, *goal_list_p4, *goal_list_p5, *goal_list_p6, *goal_list_p7, *goal_list_p8, *goal_list_p9, *goal_list_p10]\n",
        "    if len(clue_list) > 0 and len(goal_list) > 0:\n",
        "      print(\" Clues sentence \" + str(text_sentences.index(pos_sent)) + \": \" + str(clue_list))\n",
        "      print(\" Goals sentence \" + str(text_sentences.index(pos_sent)) + \": \" + str(goal_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58ahyucx_7_Q",
        "outputId": "34020d92-6d5e-42b4-eabb-a9e8a09c867d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1:\n",
            "Text 2:\n",
            " Clues sentence 9: ['What Tim likes']\n",
            " Goals sentence 9: ['gift']\n",
            "Text 3:\n",
            "Text 4:\n",
            "Text 5:\n",
            "Text 6:\n",
            "Text 7:\n",
            "Text 8:\n",
            "Text 9:\n",
            " Clues sentence 0: ['What Jill likes']\n",
            " Goals sentence 0: ['math']\n",
            "Text 10:\n",
            "Text 11:\n",
            " Clues sentence 3: ['What Kate likes']\n",
            " Goals sentence 3: ['dog']\n",
            "Text 12:\n",
            "Text 13:\n",
            "Text 14:\n",
            "Text 15:\n",
            " Clues sentence 10: ['What Barbara likes']\n",
            " Goals sentence 10: ['one']\n",
            "Text 16:\n",
            " Clues sentence 2: ['Her mom takes a ...']\n",
            " Goals sentence 2: ['dentist']\n",
            "Text 17:\n",
            "Text 18:\n",
            "Text 19:\n",
            "Text 20:\n",
            "Text 21:\n",
            "Text 22:\n",
            "Text 23:\n",
            "Text 24:\n",
            "Text 25:\n",
            "Text 26:\n",
            "Text 27:\n",
            "Text 28:\n",
            "Text 29:\n",
            "Text 30:\n",
            "Text 31:\n",
            "Text 32:\n",
            "Text 33:\n",
            "Text 34:\n",
            "Text 35:\n",
            "Text 36:\n",
            " Clues sentence 2: ['from Canada house is ...']\n",
            " Goals sentence 2: ['Nevada ']\n",
            "Text 37:\n",
            "Text 38:\n",
            "Text 39:\n",
            " Clues sentence 9: ['What one likes']\n",
            " Goals sentence 9: ['flu']\n",
            "Text 40:\n",
            "Text 41:\n",
            "Text 42:\n",
            "Text 43:\n",
            "Text 44:\n",
            "Text 45:\n",
            "Text 46:\n",
            "Text 47:\n",
            "Text 48:\n",
            "Text 49:\n",
            "Text 50:\n",
            "Text 51:\n",
            "Text 52:\n",
            "Text 53:\n",
            "Text 54:\n",
            "Text 55:\n",
            "Text 56:\n",
            "Text 57:\n",
            "Text 58:\n",
            "Text 59:\n",
            "Text 60:\n",
            "Text 61:\n",
            "Text 62:\n",
            "Text 63:\n",
            "Text 64:\n",
            " Clues sentence 2: ['His aunt is ...', 'His aunt is skinny and ...']\n",
            " Goals sentence 2: ['skinny ', 'tall ']\n",
            " Clues sentence 3: ['His uncle is ...', 'His uncle is short and ...']\n",
            " Goals sentence 3: ['short ', 'fat ']\n",
            " Clues sentence 4: ['What His aunt likes']\n",
            " Goals sentence 4: ['vegetables']\n",
            "Text 65:\n",
            "Text 66:\n",
            "Text 67:\n",
            "Text 68:\n",
            "Text 69:\n",
            "Text 70:\n",
            "Text 71:\n",
            "Text 72:\n",
            "Text 73:\n",
            "Text 74:\n",
            " Clues sentence 2: ['My mom likes ...', 'My mom likes french fries and ...', 'What My mom likes', 'What My mom likes']\n",
            " Goals sentence 2: ['french fries ', 'hamburgers ', 'hamburgers', 'fries']\n",
            " Clues sentence 4: ['What My dad likes', 'What My dad likes']\n",
            " Goals sentence 4: ['lemons', 'grapefruits']\n",
            " Clues sentence 6: ['What My sister likes', 'What My sister likes']\n",
            " Goals sentence 6: ['coffee', 'chocolate']\n",
            " Clues sentence 8: ['What My brother likes']\n",
            " Goals sentence 8: ['everything']\n",
            "Text 75:\n",
            "Text 76:\n",
            "Text 77:\n",
            "Text 78:\n",
            "Text 79:\n",
            "Text 80:\n",
            "Text 81:\n",
            "Text 82:\n",
            "Text 83:\n",
            "Text 84:\n",
            "Text 85:\n",
            "Text 86:\n",
            "Text 87:\n",
            "Text 88:\n",
            "Text 89:\n",
            "Text 90:\n",
            "Text 91:\n",
            "Text 92:\n",
            "Text 93:\n",
            "Text 94:\n",
            "Text 95:\n",
            "Text 96:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ejecutamos patrones sobre dataset \"readworks\""
      ],
      "metadata": {
        "id": "3rPKUIdM_b6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este dataset está formado por textos un poco más complejos, en donde los patrones implementados no funcionan muy bien."
      ],
      "metadata": {
        "id": "55c193n9_fp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/Semestre Impar 2022/Modulos PLN/ME/dataset-readworks/\"\n",
        "all_texts_sentences, filenames = read_dataset_files_sentences(DATASET_PATH)\n",
        "texts_pos_sentences = get_pos_features_sentences(all_texts_sentences)"
      ],
      "metadata": {
        "id": "M4s4siFv_qPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text_sentences in texts_pos_sentences:\n",
        "  print(\"Text \" + str(texts_pos_sentences.index(text_sentences) + 1) + \":\")\n",
        "  for pos_sent in text_sentences:\n",
        "    tokens, pos = separate_tuples(pos_sent)\n",
        "    clue_list_p1, goal_list_p1 = pattern_1(tokens, pos, pos_sent)\n",
        "    clue_list_p2, goal_list_p2 = pattern_2(tokens, pos, pos_sent)\n",
        "    clue_list_p3, goal_list_p3 = pattern_3(tokens, pos, pos_sent)\n",
        "    clue_list_p4, goal_list_p4 = pattern_4(tokens, pos, pos_sent)\n",
        "    clue_list_p5, goal_list_p5 = pattern_5(tokens, pos, pos_sent)\n",
        "    clue_list_p6, goal_list_p6 = pattern_6(tokens, pos, pos_sent)\n",
        "    clue_list_p7, goal_list_p7 = pattern_7(tokens, pos, pos_sent)\n",
        "    clue_list_p8, goal_list_p8 = pattern_8(tokens, pos, pos_sent)\n",
        "    clue_list_p9, goal_list_p9 = pattern_9(tokens, pos, pos_sent)\n",
        "    clue_list_p10, goal_list_p10 = pattern_10(tokens, pos, pos_sent)\n",
        "    clue_list = [*clue_list_p1, *clue_list_p2, *clue_list_p3, *clue_list_p4, *clue_list_p5, *clue_list_p6, *clue_list_p7, *clue_list_p8, *clue_list_p9, *clue_list_p10] \n",
        "    goal_list = [*goal_list_p1, *goal_list_p2, *goal_list_p3, *goal_list_p4, *goal_list_p5, *goal_list_p6, *goal_list_p7, *goal_list_p8, *goal_list_p9, *goal_list_p10]\n",
        "    if len(clue_list) > 0 and len(goal_list) > 0:\n",
        "      print(\" Clues sentence \" + str(text_sentences.index(pos_sent)) + \": \" + str(clue_list))\n",
        "      print(\" Goals sentence \" + str(text_sentences.index(pos_sent)) + \": \" + str(goal_list))"
      ],
      "metadata": {
        "id": "PHOBZd2U_rDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}